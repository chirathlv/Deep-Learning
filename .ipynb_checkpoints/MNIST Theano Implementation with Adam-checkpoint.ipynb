{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano implementation of MNIST Handwritten Digits Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import theano as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import sys, os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Training and Test data\n",
    "df = pd.read_csv(\"..\\..\\DataSet\\mnist_train.csv\", encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output Split\n",
    "X = df.iloc[:,1:].to_numpy() / 255\n",
    "Y = df.iloc[:,0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test split\n",
    "Xtrain, Xtest, Ytrain, Ytest = X[:-1000], X[-1000:],Y[:-1000],Y[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data prior training\n",
    "Xtrain, Ytrain = shuffle(Xtrain, Ytrain, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image func\n",
    "def showImage(x, y):\n",
    "    plt.title(\"Digit: \" + str(y))\n",
    "    plt.imshow(x.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample photo\n",
    "photo = 0\n",
    "showImage(Xtrain[photo], Ytrain[photo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(y):\n",
    "    # Target index matrix from Ytrain\n",
    "    T = np.zeros((y.shape[0], np.unique(y).shape[0]))\n",
    "\n",
    "    for i,val in enumerate(y):\n",
    "        T[i,val] = 1\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, yhat):\n",
    "    return np.mean(y==yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | Batch: 1 | Cost: 6.071600744357522 | Accuracy: 15.166101694915254%\n",
      "Iteration: 0 | Batch: 2 | Cost: 4.912615054798073 | Accuracy: 19.80508474576271%\n",
      "Iteration: 0 | Batch: 3 | Cost: 3.9862110780144144 | Accuracy: 25.73050847457627%\n",
      "Iteration: 0 | Batch: 4 | Cost: 3.1982770619061354 | Accuracy: 31.105084745762714%\n",
      "Iteration: 0 | Batch: 5 | Cost: 2.7687380958814622 | Accuracy: 37.47118644067797%\n",
      "Iteration: 1 | Batch: 1 | Cost: 2.372545684313313 | Accuracy: 40.44915254237288%\n",
      "Iteration: 1 | Batch: 2 | Cost: 2.1111882626516576 | Accuracy: 45.545762711864406%\n",
      "Iteration: 1 | Batch: 3 | Cost: 1.8886591991329496 | Accuracy: 48.11864406779661%\n",
      "Iteration: 1 | Batch: 4 | Cost: 1.629250029092837 | Accuracy: 52.80508474576271%\n",
      "Iteration: 1 | Batch: 5 | Cost: 1.5519216075812103 | Accuracy: 55.052542372881355%\n",
      "Iteration: 2 | Batch: 1 | Cost: 1.404242167558447 | Accuracy: 58.63898305084746%\n",
      "Iteration: 2 | Batch: 2 | Cost: 1.2734558524693305 | Accuracy: 59.747457627118635%\n",
      "Iteration: 2 | Batch: 3 | Cost: 1.157974003223727 | Accuracy: 62.28135593220338%\n",
      "Iteration: 2 | Batch: 4 | Cost: 1.016872115155056 | Accuracy: 64.5271186440678%\n",
      "Iteration: 2 | Batch: 5 | Cost: 0.9725105271275565 | Accuracy: 66.04745762711865%\n",
      "Iteration: 3 | Batch: 1 | Cost: 0.9255974893505832 | Accuracy: 67.73898305084745%\n",
      "Iteration: 3 | Batch: 2 | Cost: 0.8642299394435338 | Accuracy: 68.88474576271186%\n",
      "Iteration: 3 | Batch: 3 | Cost: 0.8078748371762412 | Accuracy: 70.01016949152542%\n",
      "Iteration: 3 | Batch: 4 | Cost: 0.7371937656435358 | Accuracy: 71.12203389830508%\n",
      "Iteration: 3 | Batch: 5 | Cost: 0.7284619304250352 | Accuracy: 71.95423728813559%\n",
      "Iteration: 4 | Batch: 1 | Cost: 0.7111256611965507 | Accuracy: 73.2%\n",
      "Iteration: 4 | Batch: 2 | Cost: 0.6730515156269125 | Accuracy: 73.83728813559321%\n",
      "Iteration: 4 | Batch: 3 | Cost: 0.6436517145853469 | Accuracy: 74.61694915254238%\n",
      "Iteration: 4 | Batch: 4 | Cost: 0.5908898115697185 | Accuracy: 75.31525423728813%\n",
      "Iteration: 4 | Batch: 5 | Cost: 0.5849733552276746 | Accuracy: 75.72881355932203%\n",
      "Iteration: 5 | Batch: 1 | Cost: 0.586853056874739 | Accuracy: 76.74406779661017%\n",
      "Iteration: 5 | Batch: 2 | Cost: 0.5564994248911304 | Accuracy: 77.18813559322034%\n",
      "Iteration: 5 | Batch: 3 | Cost: 0.5406172788277417 | Accuracy: 77.61694915254238%\n",
      "Iteration: 5 | Batch: 4 | Cost: 0.4978920232374492 | Accuracy: 78.2593220338983%\n",
      "Iteration: 5 | Batch: 5 | Cost: 0.4878228536220554 | Accuracy: 78.74237288135593%\n",
      "Iteration: 6 | Batch: 1 | Cost: 0.4999636786701416 | Accuracy: 79.11694915254238%\n",
      "Iteration: 6 | Batch: 2 | Cost: 0.47798568683378867 | Accuracy: 79.48135593220339%\n",
      "Iteration: 6 | Batch: 3 | Cost: 0.4709733366114629 | Accuracy: 79.69830508474575%\n",
      "Iteration: 6 | Batch: 4 | Cost: 0.44385421258191593 | Accuracy: 79.79322033898305%\n",
      "Iteration: 6 | Batch: 5 | Cost: 0.44215178718701964 | Accuracy: 79.83559322033898%\n",
      "Iteration: 7 | Batch: 1 | Cost: 0.4589222317647871 | Accuracy: 80.5728813559322%\n",
      "Iteration: 7 | Batch: 2 | Cost: 0.4394699098203866 | Accuracy: 80.7728813559322%\n",
      "Iteration: 7 | Batch: 3 | Cost: 0.43399756376465526 | Accuracy: 81.22203389830509%\n",
      "Iteration: 7 | Batch: 4 | Cost: 0.39699895748722586 | Accuracy: 82.13389830508474%\n",
      "Iteration: 7 | Batch: 5 | Cost: 0.37350194936765135 | Accuracy: 82.45254237288135%\n",
      "Iteration: 8 | Batch: 1 | Cost: 0.3979490025732188 | Accuracy: 82.42033898305084%\n",
      "Iteration: 8 | Batch: 2 | Cost: 0.38244137001201656 | Accuracy: 82.75423728813558%\n",
      "Iteration: 8 | Batch: 3 | Cost: 0.37789166178864286 | Accuracy: 82.65423728813559%\n",
      "Iteration: 8 | Batch: 4 | Cost: 0.3617859748011999 | Accuracy: 82.98474576271187%\n",
      "Iteration: 8 | Batch: 5 | Cost: 0.3511797949898206 | Accuracy: 83.0322033898305%\n",
      "Iteration: 9 | Batch: 1 | Cost: 0.36467836068718473 | Accuracy: 83.95762711864407%\n",
      "Iteration: 9 | Batch: 2 | Cost: 0.3401214736286985 | Accuracy: 84.08474576271186%\n",
      "Iteration: 9 | Batch: 3 | Cost: 0.33852536626233887 | Accuracy: 84.4271186440678%\n",
      "Iteration: 9 | Batch: 4 | Cost: 0.31379621085363435 | Accuracy: 84.64067796610169%\n",
      "Iteration: 9 | Batch: 5 | Cost: 0.30046550339950445 | Accuracy: 84.9186440677966%\n",
      "Iteration: 10 | Batch: 1 | Cost: 0.32531427512646527 | Accuracy: 84.67966101694915%\n",
      "Iteration: 10 | Batch: 2 | Cost: 0.3142020925485439 | Accuracy: 85.06271186440678%\n",
      "Iteration: 10 | Batch: 3 | Cost: 0.3130352442008071 | Accuracy: 84.95254237288135%\n",
      "Iteration: 10 | Batch: 4 | Cost: 0.29723932647075846 | Accuracy: 85.24406779661017%\n",
      "Iteration: 10 | Batch: 5 | Cost: 0.2878490877149885 | Accuracy: 85.08474576271186%\n",
      "Iteration: 11 | Batch: 1 | Cost: 0.303274913877399 | Accuracy: 85.85254237288136%\n",
      "Iteration: 11 | Batch: 2 | Cost: 0.28329171823989 | Accuracy: 85.85254237288136%\n",
      "Iteration: 11 | Batch: 3 | Cost: 0.28549050517597746 | Accuracy: 86.00169491525423%\n",
      "Iteration: 11 | Batch: 4 | Cost: 0.2668105014729325 | Accuracy: 86.18305084745764%\n",
      "Iteration: 11 | Batch: 5 | Cost: 0.257046297440224 | Accuracy: 86.39830508474576%\n",
      "Iteration: 12 | Batch: 1 | Cost: 0.27934457803760393 | Accuracy: 86.00169491525423%\n",
      "Iteration: 12 | Batch: 2 | Cost: 0.27000313283544913 | Accuracy: 86.37966101694914%\n",
      "Iteration: 12 | Batch: 3 | Cost: 0.27307492677960865 | Accuracy: 86.28305084745763%\n",
      "Iteration: 12 | Batch: 4 | Cost: 0.2574408751324447 | Accuracy: 86.54915254237288%\n",
      "Iteration: 12 | Batch: 5 | Cost: 0.2494925496178832 | Accuracy: 86.42542372881356%\n",
      "Iteration: 13 | Batch: 1 | Cost: 0.26109975968599614 | Accuracy: 87.07966101694915%\n",
      "Iteration: 13 | Batch: 2 | Cost: 0.2454299017346953 | Accuracy: 87.15423728813559%\n",
      "Iteration: 13 | Batch: 3 | Cost: 0.2503271846588559 | Accuracy: 87.11864406779661%\n",
      "Iteration: 13 | Batch: 4 | Cost: 0.235089076764518 | Accuracy: 87.27796610169491%\n",
      "Iteration: 13 | Batch: 5 | Cost: 0.22704619545823776 | Accuracy: 87.49152542372882%\n",
      "Iteration: 14 | Batch: 1 | Cost: 0.24676812698743333 | Accuracy: 87.01186440677967%\n",
      "Iteration: 14 | Batch: 2 | Cost: 0.2387779377658401 | Accuracy: 87.3406779661017%\n",
      "Iteration: 14 | Batch: 3 | Cost: 0.243770852661043 | Accuracy: 87.40847457627119%\n",
      "Iteration: 14 | Batch: 4 | Cost: 0.22718616302834532 | Accuracy: 87.64745762711864%\n",
      "Iteration: 14 | Batch: 5 | Cost: 0.21964175437054897 | Accuracy: 87.64745762711864%\n",
      "Iteration: 15 | Batch: 1 | Cost: 0.22899354598419355 | Accuracy: 88.19830508474577%\n",
      "Iteration: 15 | Batch: 2 | Cost: 0.21391532131656646 | Accuracy: 88.23220338983052%\n",
      "Iteration: 15 | Batch: 3 | Cost: 0.22066776688111606 | Accuracy: 88.23728813559322%\n",
      "Iteration: 15 | Batch: 4 | Cost: 0.20818370491062135 | Accuracy: 88.32372881355933%\n",
      "Iteration: 15 | Batch: 5 | Cost: 0.19931819884427587 | Accuracy: 88.49491525423728%\n",
      "Iteration: 16 | Batch: 1 | Cost: 0.21670253022969982 | Accuracy: 88.22881355932203%\n",
      "Iteration: 16 | Batch: 2 | Cost: 0.20634082604142967 | Accuracy: 88.46271186440678%\n",
      "Iteration: 16 | Batch: 3 | Cost: 0.21266923728103385 | Accuracy: 88.52372881355932%\n",
      "Iteration: 16 | Batch: 4 | Cost: 0.20054182503158838 | Accuracy: 88.55084745762711%\n",
      "Iteration: 16 | Batch: 5 | Cost: 0.19361743215120436 | Accuracy: 88.59152542372881%\n",
      "Iteration: 17 | Batch: 1 | Cost: 0.2037253751500599 | Accuracy: 89.03728813559322%\n",
      "Iteration: 17 | Batch: 2 | Cost: 0.18932736565630223 | Accuracy: 89.02372881355932%\n",
      "Iteration: 17 | Batch: 3 | Cost: 0.19796002225027282 | Accuracy: 88.95593220338984%\n",
      "Iteration: 17 | Batch: 4 | Cost: 0.1880803005073644 | Accuracy: 89.03559322033898%\n",
      "Iteration: 17 | Batch: 5 | Cost: 0.17808645873423976 | Accuracy: 89.24406779661017%\n",
      "Iteration: 18 | Batch: 1 | Cost: 0.19496980398823974 | Accuracy: 88.94745762711864%\n",
      "Iteration: 18 | Batch: 2 | Cost: 0.1859586047006075 | Accuracy: 89.04237288135593%\n",
      "Iteration: 18 | Batch: 3 | Cost: 0.19412768276725217 | Accuracy: 89.16610169491526%\n",
      "Iteration: 18 | Batch: 4 | Cost: 0.1844606249425697 | Accuracy: 89.04915254237288%\n",
      "Iteration: 18 | Batch: 5 | Cost: 0.18063885208649305 | Accuracy: 89.01694915254237%\n",
      "Iteration: 19 | Batch: 1 | Cost: 0.19300245958869458 | Accuracy: 89.02203389830508%\n",
      "Iteration: 19 | Batch: 2 | Cost: 0.190169321476265 | Accuracy: 88.83050847457628%\n",
      "Iteration: 19 | Batch: 3 | Cost: 0.20024149646131986 | Accuracy: 88.66949152542372%\n",
      "Iteration: 19 | Batch: 4 | Cost: 0.1945819306728561 | Accuracy: 89.21525423728814%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19 | Batch: 5 | Cost: 0.17377032426522493 | Accuracy: 89.49322033898305%\n",
      "Iteration: 20 | Batch: 1 | Cost: 0.1887573178531048 | Accuracy: 89.33898305084746%\n",
      "Iteration: 20 | Batch: 2 | Cost: 0.17709321828032207 | Accuracy: 89.51525423728813%\n",
      "Iteration: 20 | Batch: 3 | Cost: 0.18427640730113679 | Accuracy: 89.9186440677966%\n",
      "Iteration: 20 | Batch: 4 | Cost: 0.16809190402432142 | Accuracy: 90.0406779661017%\n",
      "Iteration: 20 | Batch: 5 | Cost: 0.16022359954782606 | Accuracy: 90.13220338983051%\n",
      "Iteration: 21 | Batch: 1 | Cost: 0.1696672683833384 | Accuracy: 90.16440677966102%\n",
      "Iteration: 21 | Batch: 2 | Cost: 0.1592909229063062 | Accuracy: 90.12372881355932%\n",
      "Iteration: 21 | Batch: 3 | Cost: 0.16942917080922212 | Accuracy: 90.19661016949152%\n",
      "Iteration: 21 | Batch: 4 | Cost: 0.15937709364559893 | Accuracy: 90.3864406779661%\n",
      "Iteration: 21 | Batch: 5 | Cost: 0.1483936944237785 | Accuracy: 90.5271186440678%\n",
      "Iteration: 22 | Batch: 1 | Cost: 0.15928115631663603 | Accuracy: 90.59491525423728%\n",
      "Iteration: 22 | Batch: 2 | Cost: 0.14812787768852473 | Accuracy: 90.6457627118644%\n",
      "Iteration: 22 | Batch: 3 | Cost: 0.15673833308335577 | Accuracy: 90.71694915254237%\n",
      "Iteration: 22 | Batch: 4 | Cost: 0.14852090218813968 | Accuracy: 90.81694915254236%\n",
      "Iteration: 22 | Batch: 5 | Cost: 0.1403809214727883 | Accuracy: 90.76610169491526%\n",
      "Iteration: 23 | Batch: 1 | Cost: 0.15269519762246175 | Accuracy: 90.65423728813559%\n",
      "Iteration: 23 | Batch: 2 | Cost: 0.1427213377976742 | Accuracy: 90.7593220338983%\n",
      "Iteration: 23 | Batch: 3 | Cost: 0.15335195228080725 | Accuracy: 90.6728813559322%\n",
      "Iteration: 23 | Batch: 4 | Cost: 0.14555088770805263 | Accuracy: 90.77796610169491%\n",
      "Iteration: 23 | Batch: 5 | Cost: 0.13691772791293397 | Accuracy: 90.87966101694916%\n",
      "Iteration: 24 | Batch: 1 | Cost: 0.14640016438241923 | Accuracy: 91.05254237288135%\n",
      "Iteration: 24 | Batch: 2 | Cost: 0.13559518566249018 | Accuracy: 90.9728813559322%\n",
      "Iteration: 24 | Batch: 3 | Cost: 0.14451824820887876 | Accuracy: 91.00169491525423%\n",
      "Iteration: 24 | Batch: 4 | Cost: 0.13867411886582956 | Accuracy: 91.05084745762711%\n",
      "Iteration: 24 | Batch: 5 | Cost: 0.13042696166246923 | Accuracy: 91.07796610169491%\n",
      "Iteration: 25 | Batch: 1 | Cost: 0.14507225361957038 | Accuracy: 90.59830508474576%\n",
      "Iteration: 25 | Batch: 2 | Cost: 0.13787011821988224 | Accuracy: 90.59152542372881%\n",
      "Iteration: 25 | Batch: 3 | Cost: 0.15094548714828795 | Accuracy: 90.72542372881355%\n",
      "Iteration: 25 | Batch: 4 | Cost: 0.1401463797060711 | Accuracy: 90.71525423728814%\n",
      "Iteration: 25 | Batch: 5 | Cost: 0.13458806950696425 | Accuracy: 90.7135593220339%\n",
      "Iteration: 26 | Batch: 1 | Cost: 0.14195501123104654 | Accuracy: 91.28474576271186%\n",
      "Iteration: 26 | Batch: 2 | Cost: 0.12893128468551115 | Accuracy: 91.1728813559322%\n",
      "Iteration: 26 | Batch: 3 | Cost: 0.13775918851433294 | Accuracy: 91.23728813559322%\n",
      "Iteration: 26 | Batch: 4 | Cost: 0.1316464772077865 | Accuracy: 91.13559322033899%\n",
      "Iteration: 26 | Batch: 5 | Cost: 0.12354547882569333 | Accuracy: 91.3271186440678%\n",
      "Iteration: 27 | Batch: 1 | Cost: 0.13752251984251576 | Accuracy: 90.96101694915254%\n",
      "Iteration: 27 | Batch: 2 | Cost: 0.1282274325008505 | Accuracy: 90.97966101694915%\n",
      "Iteration: 27 | Batch: 3 | Cost: 0.13915625424470207 | Accuracy: 91.07796610169491%\n",
      "Iteration: 27 | Batch: 4 | Cost: 0.13109471871070422 | Accuracy: 91.09830508474576%\n",
      "Iteration: 27 | Batch: 5 | Cost: 0.12476605954450774 | Accuracy: 91.01694915254237%\n",
      "Iteration: 28 | Batch: 1 | Cost: 0.1329830348724207 | Accuracy: 91.68474576271186%\n",
      "Iteration: 28 | Batch: 2 | Cost: 0.11919509540751745 | Accuracy: 91.45084745762712%\n",
      "Iteration: 28 | Batch: 3 | Cost: 0.12957717300028465 | Accuracy: 91.44237288135592%\n",
      "Iteration: 28 | Batch: 4 | Cost: 0.12380244073123255 | Accuracy: 91.5542372881356%\n",
      "Iteration: 28 | Batch: 5 | Cost: 0.1145698016582785 | Accuracy: 91.69830508474577%\n",
      "Iteration: 29 | Batch: 1 | Cost: 0.12688321043645837 | Accuracy: 91.5542372881356%\n",
      "Iteration: 29 | Batch: 2 | Cost: 0.11743441157605272 | Accuracy: 91.53559322033898%\n",
      "Iteration: 29 | Batch: 3 | Cost: 0.12556051783696068 | Accuracy: 91.79322033898305%\n",
      "Iteration: 29 | Batch: 4 | Cost: 0.11901345012399267 | Accuracy: 91.59322033898304%\n",
      "Iteration: 29 | Batch: 5 | Cost: 0.11525046123628693 | Accuracy: 91.64067796610169%\n",
      "Iteration: 30 | Batch: 1 | Cost: 0.123880906564187 | Accuracy: 91.65593220338984%\n",
      "Iteration: 30 | Batch: 2 | Cost: 0.11586539271536726 | Accuracy: 91.5728813559322%\n",
      "Iteration: 30 | Batch: 3 | Cost: 0.12605316059770924 | Accuracy: 91.50338983050847%\n",
      "Iteration: 30 | Batch: 4 | Cost: 0.12183666224680165 | Accuracy: 91.83559322033898%\n",
      "Iteration: 30 | Batch: 5 | Cost: 0.1078953104520421 | Accuracy: 91.95593220338984%\n",
      "Iteration: 31 | Batch: 1 | Cost: 0.12072818634467646 | Accuracy: 91.84576271186441%\n",
      "Iteration: 31 | Batch: 2 | Cost: 0.10984053002463896 | Accuracy: 91.95084745762712%\n",
      "Iteration: 31 | Batch: 3 | Cost: 0.11764629578958412 | Accuracy: 92.24406779661017%\n",
      "Iteration: 31 | Batch: 4 | Cost: 0.11011389698857388 | Accuracy: 92.10847457627118%\n",
      "Iteration: 31 | Batch: 5 | Cost: 0.1051615087317463 | Accuracy: 92.19322033898305%\n",
      "Iteration: 32 | Batch: 1 | Cost: 0.11369378477846719 | Accuracy: 92.1593220338983%\n",
      "Iteration: 32 | Batch: 2 | Cost: 0.10410664576331703 | Accuracy: 92.23050847457627%\n",
      "Iteration: 32 | Batch: 3 | Cost: 0.11183640899435394 | Accuracy: 92.2186440677966%\n",
      "Iteration: 32 | Batch: 4 | Cost: 0.10667842559115119 | Accuracy: 92.48474576271187%\n",
      "Iteration: 32 | Batch: 5 | Cost: 0.0962919304725455 | Accuracy: 92.70338983050847%\n",
      "Iteration: 33 | Batch: 1 | Cost: 0.10575448412116488 | Accuracy: 92.64067796610169%\n",
      "Iteration: 33 | Batch: 2 | Cost: 0.09528767085399435 | Accuracy: 92.67966101694915%\n",
      "Iteration: 33 | Batch: 3 | Cost: 0.10452746303094533 | Accuracy: 92.75254237288135%\n",
      "Iteration: 33 | Batch: 4 | Cost: 0.09862380356130765 | Accuracy: 92.7322033898305%\n",
      "Iteration: 33 | Batch: 5 | Cost: 0.09223345194982685 | Accuracy: 92.66101694915254%\n",
      "Iteration: 34 | Batch: 1 | Cost: 0.10262964156711155 | Accuracy: 92.70847457627119%\n",
      "Iteration: 34 | Batch: 2 | Cost: 0.09181743170902376 | Accuracy: 92.79322033898305%\n",
      "Iteration: 34 | Batch: 3 | Cost: 0.10062042617606903 | Accuracy: 92.75762711864407%\n",
      "Iteration: 34 | Batch: 4 | Cost: 0.09525847072634178 | Accuracy: 92.92033898305084%\n",
      "Iteration: 34 | Batch: 5 | Cost: 0.08734534366570935 | Accuracy: 93.0135593220339%\n",
      "Iteration: 35 | Batch: 1 | Cost: 0.09696655282928641 | Accuracy: 92.99152542372882%\n",
      "Iteration: 35 | Batch: 2 | Cost: 0.08635134345689456 | Accuracy: 93.02881355932203%\n",
      "Iteration: 35 | Batch: 3 | Cost: 0.09640508947514928 | Accuracy: 93.0406779661017%\n",
      "Iteration: 35 | Batch: 4 | Cost: 0.09146162545884437 | Accuracy: 93.01694915254237%\n",
      "Iteration: 35 | Batch: 5 | Cost: 0.08547778404721555 | Accuracy: 92.84406779661018%\n",
      "Iteration: 36 | Batch: 1 | Cost: 0.09626770792865799 | Accuracy: 92.89152542372882%\n",
      "Iteration: 36 | Batch: 2 | Cost: 0.08605427137050058 | Accuracy: 92.87966101694916%\n",
      "Iteration: 36 | Batch: 3 | Cost: 0.09545157863888157 | Accuracy: 92.84745762711864%\n",
      "Iteration: 36 | Batch: 4 | Cost: 0.09104912553619456 | Accuracy: 92.94745762711865%\n",
      "Iteration: 36 | Batch: 5 | Cost: 0.08352543000805457 | Accuracy: 93.05254237288135%\n",
      "Iteration: 37 | Batch: 1 | Cost: 0.09383660214261448 | Accuracy: 92.87796610169492%\n",
      "Iteration: 37 | Batch: 2 | Cost: 0.08508495979559422 | Accuracy: 92.68983050847459%\n",
      "Iteration: 37 | Batch: 3 | Cost: 0.09762463244733131 | Accuracy: 92.60169491525424%\n",
      "Iteration: 37 | Batch: 4 | Cost: 0.09533775541247722 | Accuracy: 92.37118644067796%\n",
      "Iteration: 37 | Batch: 5 | Cost: 0.09220191728237571 | Accuracy: 92.23389830508475%\n",
      "Iteration: 38 | Batch: 1 | Cost: 0.10368989387854996 | Accuracy: 92.0271186440678%\n",
      "Iteration: 38 | Batch: 2 | Cost: 0.09756072041167471 | Accuracy: 91.93220338983052%\n",
      "Iteration: 38 | Batch: 3 | Cost: 0.10909521032269962 | Accuracy: 91.88474576271186%\n",
      "Iteration: 38 | Batch: 4 | Cost: 0.10719051258155546 | Accuracy: 92.30847457627118%\n",
      "Iteration: 38 | Batch: 5 | Cost: 0.09204859819276381 | Accuracy: 92.54576271186441%\n",
      "Iteration: 39 | Batch: 1 | Cost: 0.10078401046638845 | Accuracy: 92.50847457627118%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 39 | Batch: 2 | Cost: 0.09054336207923412 | Accuracy: 92.54406779661018%\n",
      "Iteration: 39 | Batch: 3 | Cost: 0.09928424954868266 | Accuracy: 92.83389830508474%\n",
      "Iteration: 39 | Batch: 4 | Cost: 0.09240450530027751 | Accuracy: 92.9728813559322%\n",
      "Iteration: 39 | Batch: 5 | Cost: 0.08314257483623656 | Accuracy: 93.10508474576271%\n",
      "Iteration: 40 | Batch: 1 | Cost: 0.09106826296762778 | Accuracy: 93.05254237288135%\n",
      "Iteration: 40 | Batch: 2 | Cost: 0.07896270891143492 | Accuracy: 93.4%\n",
      "Iteration: 40 | Batch: 3 | Cost: 0.08660102107123238 | Accuracy: 93.42542372881356%\n",
      "Iteration: 40 | Batch: 4 | Cost: 0.08147684895578011 | Accuracy: 93.63559322033899%\n",
      "Iteration: 40 | Batch: 5 | Cost: 0.07256145270218167 | Accuracy: 93.59152542372881%\n",
      "Iteration: 41 | Batch: 1 | Cost: 0.08100070203530108 | Accuracy: 93.79152542372881%\n",
      "Iteration: 41 | Batch: 2 | Cost: 0.06994585901999427 | Accuracy: 93.66779661016949%\n",
      "Iteration: 41 | Batch: 3 | Cost: 0.08139865681876202 | Accuracy: 93.63220338983051%\n",
      "Iteration: 41 | Batch: 4 | Cost: 0.07757210566398216 | Accuracy: 93.7186440677966%\n",
      "Iteration: 41 | Batch: 5 | Cost: 0.06946630908418858 | Accuracy: 93.78983050847458%\n",
      "Iteration: 42 | Batch: 1 | Cost: 0.07992756532830948 | Accuracy: 93.39830508474576%\n",
      "Iteration: 42 | Batch: 2 | Cost: 0.07123963660030126 | Accuracy: 93.35084745762711%\n",
      "Iteration: 42 | Batch: 3 | Cost: 0.0833848984419622 | Accuracy: 93.29661016949153%\n",
      "Iteration: 42 | Batch: 4 | Cost: 0.08018053724075695 | Accuracy: 93.15593220338984%\n",
      "Iteration: 42 | Batch: 5 | Cost: 0.0763812201847289 | Accuracy: 92.68305084745762%\n",
      "Iteration: 43 | Batch: 1 | Cost: 0.08762234727249714 | Accuracy: 92.97796610169492%\n",
      "Iteration: 43 | Batch: 2 | Cost: 0.07772126775283092 | Accuracy: 92.81694915254238%\n",
      "Iteration: 43 | Batch: 3 | Cost: 0.09024254638630229 | Accuracy: 92.99830508474575%\n",
      "Iteration: 43 | Batch: 4 | Cost: 0.08265991830159339 | Accuracy: 92.9864406779661%\n",
      "Iteration: 43 | Batch: 5 | Cost: 0.07512976277993365 | Accuracy: 93.10508474576271%\n",
      "Iteration: 44 | Batch: 1 | Cost: 0.09002748503926458 | Accuracy: 92.33559322033898%\n",
      "Iteration: 44 | Batch: 2 | Cost: 0.08643736855691288 | Accuracy: 91.88135593220339%\n",
      "Iteration: 44 | Batch: 3 | Cost: 0.10121638969774278 | Accuracy: 91.86779661016949%\n",
      "Iteration: 44 | Batch: 4 | Cost: 0.10018092337423605 | Accuracy: 91.6542372881356%\n",
      "Iteration: 44 | Batch: 5 | Cost: 0.099428249118115 | Accuracy: 91.35762711864407%\n",
      "Iteration: 45 | Batch: 1 | Cost: 0.10293159727038519 | Accuracy: 92.23389830508475%\n",
      "Iteration: 45 | Batch: 2 | Cost: 0.0862699353656384 | Accuracy: 92.47627118644067%\n",
      "Iteration: 45 | Batch: 3 | Cost: 0.0932114491891783 | Accuracy: 93.1864406779661%\n",
      "Iteration: 45 | Batch: 4 | Cost: 0.07989993223862984 | Accuracy: 93.69152542372882%\n",
      "Iteration: 45 | Batch: 5 | Cost: 0.0675985093398108 | Accuracy: 94.00338983050848%\n",
      "Iteration: 46 | Batch: 1 | Cost: 0.07480030331777396 | Accuracy: 93.9864406779661%\n",
      "Iteration: 46 | Batch: 2 | Cost: 0.06295076125657578 | Accuracy: 94.08813559322033%\n",
      "Iteration: 46 | Batch: 3 | Cost: 0.07234318059086065 | Accuracy: 94.18305084745762%\n",
      "Iteration: 46 | Batch: 4 | Cost: 0.0677673815337268 | Accuracy: 94.22542372881357%\n",
      "Iteration: 46 | Batch: 5 | Cost: 0.061127798318007076 | Accuracy: 94.22203389830509%\n",
      "Iteration: 47 | Batch: 1 | Cost: 0.06985585927609567 | Accuracy: 94.20508474576272%\n",
      "Iteration: 47 | Batch: 2 | Cost: 0.058282843238311025 | Accuracy: 94.39322033898306%\n",
      "Iteration: 47 | Batch: 3 | Cost: 0.06857514344638316 | Accuracy: 94.28813559322033%\n",
      "Iteration: 47 | Batch: 4 | Cost: 0.06451918486443463 | Accuracy: 94.46949152542373%\n",
      "Iteration: 47 | Batch: 5 | Cost: 0.05653408393108004 | Accuracy: 94.47457627118644%\n",
      "Iteration: 48 | Batch: 1 | Cost: 0.06583331855906284 | Accuracy: 94.38983050847457%\n",
      "Iteration: 48 | Batch: 2 | Cost: 0.055444402884385124 | Accuracy: 94.42372881355932%\n",
      "Iteration: 48 | Batch: 3 | Cost: 0.0664897860919935 | Accuracy: 94.3542372881356%\n",
      "Iteration: 48 | Batch: 4 | Cost: 0.06310389114472867 | Accuracy: 94.40847457627119%\n",
      "Iteration: 48 | Batch: 5 | Cost: 0.056586090939956334 | Accuracy: 94.32881355932203%\n",
      "Iteration: 49 | Batch: 1 | Cost: 0.06577567656126405 | Accuracy: 94.15084745762712%\n",
      "Iteration: 49 | Batch: 2 | Cost: 0.05583042639319162 | Accuracy: 94.41186440677967%\n",
      "Iteration: 49 | Batch: 3 | Cost: 0.06600729919855367 | Accuracy: 94.20508474576272%\n",
      "Iteration: 49 | Batch: 4 | Cost: 0.06294097643141171 | Accuracy: 94.38983050847457%\n",
      "Iteration: 49 | Batch: 5 | Cost: 0.054823048940457905 | Accuracy: 94.44745762711865%\n",
      "Iteration: 50 | Batch: 1 | Cost: 0.06370365521706237 | Accuracy: 94.34406779661016%\n",
      "Iteration: 50 | Batch: 2 | Cost: 0.05487665728274306 | Accuracy: 94.15084745762712%\n",
      "Iteration: 50 | Batch: 3 | Cost: 0.06671164684600082 | Accuracy: 94.24576271186442%\n",
      "Iteration: 50 | Batch: 4 | Cost: 0.06301180064745651 | Accuracy: 94.10508474576271%\n",
      "Iteration: 50 | Batch: 5 | Cost: 0.05922708542196908 | Accuracy: 94.06271186440678%\n",
      "Iteration: 51 | Batch: 1 | Cost: 0.06824542351149379 | Accuracy: 93.51864406779661%\n",
      "Iteration: 51 | Batch: 2 | Cost: 0.06247909688070524 | Accuracy: 93.69322033898305%\n",
      "Iteration: 51 | Batch: 3 | Cost: 0.07360627289560863 | Accuracy: 93.53050847457627%\n",
      "Iteration: 51 | Batch: 4 | Cost: 0.0722463560900137 | Accuracy: 93.7%\n",
      "Iteration: 51 | Batch: 5 | Cost: 0.0619858845221536 | Accuracy: 93.82203389830508%\n",
      "Iteration: 52 | Batch: 1 | Cost: 0.0721264827941274 | Accuracy: 93.87966101694916%\n",
      "Iteration: 52 | Batch: 2 | Cost: 0.06064225308717619 | Accuracy: 93.72033898305084%\n",
      "Iteration: 52 | Batch: 3 | Cost: 0.070698130369627 | Accuracy: 94.17966101694915%\n",
      "Iteration: 52 | Batch: 4 | Cost: 0.06438841360215132 | Accuracy: 94.05254237288135%\n",
      "Iteration: 52 | Batch: 5 | Cost: 0.05901201523284636 | Accuracy: 94.31016949152541%\n",
      "Iteration: 53 | Batch: 1 | Cost: 0.06527701404110509 | Accuracy: 93.89322033898306%\n",
      "Iteration: 53 | Batch: 2 | Cost: 0.056139948102926406 | Accuracy: 94.24237288135593%\n",
      "Iteration: 53 | Batch: 3 | Cost: 0.06535449300128997 | Accuracy: 94.34745762711864%\n",
      "Iteration: 53 | Batch: 4 | Cost: 0.05990705844568476 | Accuracy: 94.66610169491526%\n",
      "Iteration: 53 | Batch: 5 | Cost: 0.05053379690345825 | Accuracy: 94.71016949152542%\n",
      "Iteration: 54 | Batch: 1 | Cost: 0.05737502928368697 | Accuracy: 94.74237288135593%\n",
      "Iteration: 54 | Batch: 2 | Cost: 0.04809724666464545 | Accuracy: 94.62203389830508%\n",
      "Iteration: 54 | Batch: 3 | Cost: 0.05904979309790683 | Accuracy: 94.70847457627119%\n",
      "Iteration: 54 | Batch: 4 | Cost: 0.055503960426621035 | Accuracy: 94.74576271186442%\n",
      "Iteration: 54 | Batch: 5 | Cost: 0.04837140951565224 | Accuracy: 94.80847457627118%\n",
      "Iteration: 55 | Batch: 1 | Cost: 0.056640321239293814 | Accuracy: 94.63050847457627%\n",
      "Iteration: 55 | Batch: 2 | Cost: 0.04641073143467082 | Accuracy: 94.59322033898306%\n",
      "Iteration: 55 | Batch: 3 | Cost: 0.05761480476130417 | Accuracy: 94.69491525423729%\n",
      "Iteration: 55 | Batch: 4 | Cost: 0.05420889757222539 | Accuracy: 94.74576271186442%\n",
      "Iteration: 55 | Batch: 5 | Cost: 0.04780889558875031 | Accuracy: 94.67627118644067%\n",
      "Iteration: 56 | Batch: 1 | Cost: 0.05559565502777915 | Accuracy: 94.72203389830509%\n",
      "Iteration: 56 | Batch: 2 | Cost: 0.045733275590832616 | Accuracy: 94.63559322033899%\n",
      "Iteration: 56 | Batch: 3 | Cost: 0.056690749164164014 | Accuracy: 94.72033898305085%\n",
      "Iteration: 56 | Batch: 4 | Cost: 0.05309526842314505 | Accuracy: 94.88813559322034%\n",
      "Iteration: 56 | Batch: 5 | Cost: 0.04523155292966846 | Accuracy: 94.94745762711865%\n",
      "Iteration: 57 | Batch: 1 | Cost: 0.05382735972040753 | Accuracy: 94.5322033898305%\n",
      "Iteration: 57 | Batch: 2 | Cost: 0.04638096207364746 | Accuracy: 94.2864406779661%\n",
      "Iteration: 57 | Batch: 3 | Cost: 0.058423738735498 | Accuracy: 94.44237288135594%\n",
      "Iteration: 57 | Batch: 4 | Cost: 0.05550561735561874 | Accuracy: 94.5%\n",
      "Iteration: 57 | Batch: 5 | Cost: 0.04923273167045056 | Accuracy: 94.26271186440678%\n",
      "Iteration: 58 | Batch: 1 | Cost: 0.05694572965718803 | Accuracy: 94.40338983050847%\n",
      "Iteration: 58 | Batch: 2 | Cost: 0.047944085208416795 | Accuracy: 94.33559322033899%\n",
      "Iteration: 58 | Batch: 3 | Cost: 0.059141881941080124 | Accuracy: 94.60169491525424%\n",
      "Iteration: 58 | Batch: 4 | Cost: 0.05301017507926052 | Accuracy: 94.87627118644068%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 58 | Batch: 5 | Cost: 0.04429191036884826 | Accuracy: 94.92033898305084%\n",
      "Iteration: 59 | Batch: 1 | Cost: 0.053668060647965365 | Accuracy: 94.26271186440678%\n",
      "Iteration: 59 | Batch: 2 | Cost: 0.04829837225968681 | Accuracy: 94.05762711864408%\n",
      "Iteration: 59 | Batch: 3 | Cost: 0.059963223451759674 | Accuracy: 94.27796610169491%\n",
      "Iteration: 59 | Batch: 4 | Cost: 0.05731552590534137 | Accuracy: 94.16440677966101%\n",
      "Iteration: 59 | Batch: 5 | Cost: 0.05177910858899922 | Accuracy: 94.06271186440678%\n",
      "Iteration: 60 | Batch: 1 | Cost: 0.05799464988098446 | Accuracy: 94.29322033898305%\n",
      "Iteration: 60 | Batch: 2 | Cost: 0.04760406176632219 | Accuracy: 94.5728813559322%\n",
      "Iteration: 60 | Batch: 3 | Cost: 0.05641656589560864 | Accuracy: 94.68305084745762%\n",
      "Iteration: 60 | Batch: 4 | Cost: 0.052693905906130614 | Accuracy: 94.95423728813559%\n",
      "Iteration: 60 | Batch: 5 | Cost: 0.043155436473792816 | Accuracy: 95.07966101694916%\n",
      "Iteration: 61 | Batch: 1 | Cost: 0.050840817412478846 | Accuracy: 94.73728813559322%\n",
      "Iteration: 61 | Batch: 2 | Cost: 0.04397274325156245 | Accuracy: 94.57966101694916%\n",
      "Iteration: 61 | Batch: 3 | Cost: 0.05391184089213432 | Accuracy: 94.7677966101695%\n",
      "Iteration: 61 | Batch: 4 | Cost: 0.05160653076550977 | Accuracy: 94.58305084745763%\n",
      "Iteration: 61 | Batch: 5 | Cost: 0.04706855107984574 | Accuracy: 94.6728813559322%\n",
      "Iteration: 62 | Batch: 1 | Cost: 0.05242808306182869 | Accuracy: 94.52372881355933%\n",
      "Iteration: 62 | Batch: 2 | Cost: 0.04315401320429482 | Accuracy: 94.92372881355932%\n",
      "Iteration: 62 | Batch: 3 | Cost: 0.05295304036556434 | Accuracy: 94.70338983050848%\n",
      "Iteration: 62 | Batch: 4 | Cost: 0.05175011009090269 | Accuracy: 95.01864406779661%\n",
      "Iteration: 62 | Batch: 5 | Cost: 0.042330890408986833 | Accuracy: 95.15593220338982%\n",
      "Iteration: 63 | Batch: 1 | Cost: 0.04870389941180968 | Accuracy: 95.04576271186441%\n",
      "Iteration: 63 | Batch: 2 | Cost: 0.04137485877373729 | Accuracy: 94.9322033898305%\n",
      "Iteration: 63 | Batch: 3 | Cost: 0.0504161209461909 | Accuracy: 95.2135593220339%\n",
      "Iteration: 63 | Batch: 4 | Cost: 0.04784206590791267 | Accuracy: 95.08474576271186%\n",
      "Iteration: 63 | Batch: 5 | Cost: 0.04205453648450741 | Accuracy: 95.25593220338983%\n",
      "Iteration: 64 | Batch: 1 | Cost: 0.04807691685196768 | Accuracy: 94.79322033898305%\n",
      "Iteration: 64 | Batch: 2 | Cost: 0.03994687110342217 | Accuracy: 95.07288135593221%\n",
      "Iteration: 64 | Batch: 3 | Cost: 0.0479652395211413 | Accuracy: 95.27966101694916%\n",
      "Iteration: 64 | Batch: 4 | Cost: 0.04479403409963774 | Accuracy: 95.47118644067797%\n",
      "Iteration: 64 | Batch: 5 | Cost: 0.0370545459811991 | Accuracy: 95.56271186440678%\n",
      "Iteration: 65 | Batch: 1 | Cost: 0.042118742873342496 | Accuracy: 95.54745762711865%\n",
      "Iteration: 65 | Batch: 2 | Cost: 0.0340831846344295 | Accuracy: 95.4135593220339%\n",
      "Iteration: 65 | Batch: 3 | Cost: 0.04412066913344477 | Accuracy: 95.56949152542373%\n",
      "Iteration: 65 | Batch: 4 | Cost: 0.04220604731069356 | Accuracy: 95.55762711864408%\n",
      "Iteration: 65 | Batch: 5 | Cost: 0.03600181298003728 | Accuracy: 95.5406779661017%\n",
      "Iteration: 66 | Batch: 1 | Cost: 0.043277299060530225 | Accuracy: 95.12881355932204%\n",
      "Iteration: 66 | Batch: 2 | Cost: 0.035576345388069044 | Accuracy: 95.13728813559322%\n",
      "Iteration: 66 | Batch: 3 | Cost: 0.0447737444481532 | Accuracy: 95.34915254237288%\n",
      "Iteration: 66 | Batch: 4 | Cost: 0.041943258459667294 | Accuracy: 95.59152542372883%\n",
      "Iteration: 66 | Batch: 5 | Cost: 0.03496776339624502 | Accuracy: 95.57288135593221%\n",
      "Iteration: 67 | Batch: 1 | Cost: 0.04011005680324155 | Accuracy: 95.64237288135593%\n",
      "Iteration: 67 | Batch: 2 | Cost: 0.03167708787202362 | Accuracy: 95.61525423728814%\n",
      "Iteration: 67 | Batch: 3 | Cost: 0.041512655878163635 | Accuracy: 95.59152542372883%\n",
      "Iteration: 67 | Batch: 4 | Cost: 0.04037648813555285 | Accuracy: 95.56610169491525%\n",
      "Iteration: 67 | Batch: 5 | Cost: 0.034439679509414484 | Accuracy: 95.54406779661016%\n",
      "Iteration: 68 | Batch: 1 | Cost: 0.042082148225229134 | Accuracy: 94.9677966101695%\n",
      "Iteration: 68 | Batch: 2 | Cost: 0.03677773852178261 | Accuracy: 94.64915254237289%\n",
      "Iteration: 68 | Batch: 3 | Cost: 0.04820491462922352 | Accuracy: 94.7271186440678%\n",
      "Iteration: 68 | Batch: 4 | Cost: 0.0470769273267443 | Accuracy: 94.89999999999999%\n",
      "Iteration: 68 | Batch: 5 | Cost: 0.041571236975884995 | Accuracy: 94.7406779661017%\n",
      "Iteration: 69 | Batch: 1 | Cost: 0.04528886510124021 | Accuracy: 95.04406779661016%\n",
      "Iteration: 69 | Batch: 2 | Cost: 0.03641730314322833 | Accuracy: 95.13389830508474%\n",
      "Iteration: 69 | Batch: 3 | Cost: 0.04572733072911167 | Accuracy: 95.16610169491526%\n",
      "Iteration: 69 | Batch: 4 | Cost: 0.04388674036042354 | Accuracy: 95.23728813559322%\n",
      "Iteration: 69 | Batch: 5 | Cost: 0.03714063819086655 | Accuracy: 95.33050847457628%\n",
      "Iteration: 70 | Batch: 1 | Cost: 0.04437670528792935 | Accuracy: 94.70847457627119%\n",
      "Iteration: 70 | Batch: 2 | Cost: 0.04010916058086063 | Accuracy: 94.38813559322034%\n",
      "Iteration: 70 | Batch: 3 | Cost: 0.05036919008793146 | Accuracy: 94.51864406779661%\n",
      "Iteration: 70 | Batch: 4 | Cost: 0.04971219183156129 | Accuracy: 94.20508474576272%\n",
      "Iteration: 70 | Batch: 5 | Cost: 0.04777819767825319 | Accuracy: 93.98474576271187%\n",
      "Iteration: 71 | Batch: 1 | Cost: 0.0525804140145556 | Accuracy: 94.28983050847458%\n",
      "Iteration: 71 | Batch: 2 | Cost: 0.042770053141555006 | Accuracy: 94.2186440677966%\n",
      "Iteration: 71 | Batch: 3 | Cost: 0.05604687429493495 | Accuracy: 94.23898305084745%\n",
      "Iteration: 71 | Batch: 4 | Cost: 0.05204712769548482 | Accuracy: 94.49152542372882%\n",
      "Iteration: 71 | Batch: 5 | Cost: 0.04431415893085599 | Accuracy: 94.67627118644067%\n",
      "Iteration: 72 | Batch: 1 | Cost: 0.05023966398629263 | Accuracy: 94.35593220338983%\n",
      "Iteration: 72 | Batch: 2 | Cost: 0.04523172611088 | Accuracy: 94.24237288135593%\n",
      "Iteration: 72 | Batch: 3 | Cost: 0.052419244847913715 | Accuracy: 94.75254237288135%\n",
      "Iteration: 72 | Batch: 4 | Cost: 0.049151971462566536 | Accuracy: 94.86949152542373%\n",
      "Iteration: 72 | Batch: 5 | Cost: 0.040993601110404385 | Accuracy: 95.26101694915255%\n",
      "Iteration: 73 | Batch: 1 | Cost: 0.043789080640438606 | Accuracy: 94.91186440677966%\n",
      "Iteration: 73 | Batch: 2 | Cost: 0.03586182571033135 | Accuracy: 95.1779661016949%\n",
      "Iteration: 73 | Batch: 3 | Cost: 0.04461579028028359 | Accuracy: 95.2949152542373%\n",
      "Iteration: 73 | Batch: 4 | Cost: 0.04169707578473838 | Accuracy: 95.62711864406779%\n",
      "Iteration: 73 | Batch: 5 | Cost: 0.03376164444301621 | Accuracy: 95.86440677966101%\n",
      "Iteration: 74 | Batch: 1 | Cost: 0.03763014870605882 | Accuracy: 95.8406779661017%\n",
      "Iteration: 74 | Batch: 2 | Cost: 0.03138389431139778 | Accuracy: 95.77118644067797%\n",
      "Iteration: 74 | Batch: 3 | Cost: 0.03828526821141301 | Accuracy: 95.90847457627119%\n",
      "Iteration: 74 | Batch: 4 | Cost: 0.03726918774391856 | Accuracy: 95.84237288135593%\n",
      "Iteration: 74 | Batch: 5 | Cost: 0.031660492713808545 | Accuracy: 95.9864406779661%\n",
      "Iteration: 75 | Batch: 1 | Cost: 0.03727664753190215 | Accuracy: 95.49322033898305%\n",
      "Iteration: 75 | Batch: 2 | Cost: 0.029386221678390702 | Accuracy: 95.85084745762713%\n",
      "Iteration: 75 | Batch: 3 | Cost: 0.036410442380984 | Accuracy: 96.07118644067796%\n",
      "Iteration: 75 | Batch: 4 | Cost: 0.034184110846656846 | Accuracy: 96.2%\n",
      "Iteration: 75 | Batch: 5 | Cost: 0.02816833305557611 | Accuracy: 96.19830508474575%\n",
      "Iteration: 76 | Batch: 1 | Cost: 0.03271906234253029 | Accuracy: 96.10508474576271%\n",
      "Iteration: 76 | Batch: 2 | Cost: 0.025747653816588983 | Accuracy: 96.05084745762711%\n",
      "Iteration: 76 | Batch: 3 | Cost: 0.03385927773265717 | Accuracy: 96.14745762711865%\n",
      "Iteration: 76 | Batch: 4 | Cost: 0.03264975060476745 | Accuracy: 96.25254237288135%\n",
      "Iteration: 76 | Batch: 5 | Cost: 0.02747201927599017 | Accuracy: 96.23050847457627%\n",
      "Iteration: 77 | Batch: 1 | Cost: 0.03281691914530505 | Accuracy: 95.8864406779661%\n",
      "Iteration: 77 | Batch: 2 | Cost: 0.025735001514650246 | Accuracy: 96.01694915254238%\n",
      "Iteration: 77 | Batch: 3 | Cost: 0.033138312317841305 | Accuracy: 96.11016949152543%\n",
      "Iteration: 77 | Batch: 4 | Cost: 0.032281425565104896 | Accuracy: 96.13898305084746%\n",
      "Iteration: 77 | Batch: 5 | Cost: 0.027143726027219938 | Accuracy: 96.16271186440677%\n",
      "Iteration: 78 | Batch: 1 | Cost: 0.03182106029389591 | Accuracy: 95.93898305084745%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 78 | Batch: 2 | Cost: 0.0256107166097078 | Accuracy: 95.94745762711865%\n",
      "Iteration: 78 | Batch: 3 | Cost: 0.03325807047734299 | Accuracy: 95.96101694915255%\n",
      "Iteration: 78 | Batch: 4 | Cost: 0.031873237954347496 | Accuracy: 96.10169491525423%\n",
      "Iteration: 78 | Batch: 5 | Cost: 0.02703164420180198 | Accuracy: 96.17966101694915%\n",
      "Iteration: 79 | Batch: 1 | Cost: 0.032446994667226496 | Accuracy: 95.53898305084746%\n",
      "Iteration: 79 | Batch: 2 | Cost: 0.027860016535892727 | Accuracy: 95.48305084745763%\n",
      "Iteration: 79 | Batch: 3 | Cost: 0.036053755156862245 | Accuracy: 95.66610169491526%\n",
      "Iteration: 79 | Batch: 4 | Cost: 0.03457417287266544 | Accuracy: 95.81864406779661%\n",
      "Iteration: 79 | Batch: 5 | Cost: 0.028974765836970638 | Accuracy: 95.7322033898305%\n",
      "Iteration: 80 | Batch: 1 | Cost: 0.034547031192537825 | Accuracy: 95.45254237288135%\n",
      "Iteration: 80 | Batch: 2 | Cost: 0.029044609464815775 | Accuracy: 95.48474576271187%\n",
      "Iteration: 80 | Batch: 3 | Cost: 0.03668858979745034 | Accuracy: 95.69661016949152%\n",
      "Iteration: 80 | Batch: 4 | Cost: 0.03333119691513965 | Accuracy: 95.92542372881357%\n",
      "Iteration: 80 | Batch: 5 | Cost: 0.02761556398407668 | Accuracy: 96.035593220339%\n",
      "Iteration: 81 | Batch: 1 | Cost: 0.03286510882797391 | Accuracy: 95.43559322033899%\n",
      "Iteration: 81 | Batch: 2 | Cost: 0.029386848631199274 | Accuracy: 95.26949152542373%\n",
      "Iteration: 81 | Batch: 3 | Cost: 0.03683905808283515 | Accuracy: 95.63898305084746%\n",
      "Iteration: 81 | Batch: 4 | Cost: 0.033897868188232774 | Accuracy: 95.95254237288135%\n",
      "Iteration: 81 | Batch: 5 | Cost: 0.02782586386346935 | Accuracy: 95.98813559322033%\n",
      "Iteration: 82 | Batch: 1 | Cost: 0.03139831541930666 | Accuracy: 95.90169491525423%\n",
      "Iteration: 82 | Batch: 2 | Cost: 0.02499631618755047 | Accuracy: 96.07288135593221%\n",
      "Iteration: 82 | Batch: 3 | Cost: 0.031894472672092423 | Accuracy: 96.01864406779661%\n",
      "Iteration: 82 | Batch: 4 | Cost: 0.030883028362104714 | Accuracy: 96.11355932203391%\n",
      "Iteration: 82 | Batch: 5 | Cost: 0.02605345049663213 | Accuracy: 96.16610169491526%\n",
      "Iteration: 83 | Batch: 1 | Cost: 0.031035948761826377 | Accuracy: 95.97457627118644%\n",
      "Iteration: 83 | Batch: 2 | Cost: 0.02543571867163616 | Accuracy: 96.02203389830508%\n",
      "Iteration: 83 | Batch: 3 | Cost: 0.03149292840073946 | Accuracy: 96.1457627118644%\n",
      "Iteration: 83 | Batch: 4 | Cost: 0.030643027252459255 | Accuracy: 95.81355932203391%\n",
      "Iteration: 83 | Batch: 5 | Cost: 0.0283798388175589 | Accuracy: 95.86101694915254%\n",
      "Iteration: 84 | Batch: 1 | Cost: 0.03427824515435472 | Accuracy: 94.9322033898305%\n",
      "Iteration: 84 | Batch: 2 | Cost: 0.033167832925712695 | Accuracy: 94.7186440677966%\n",
      "Iteration: 84 | Batch: 3 | Cost: 0.04703893750602365 | Accuracy: 94.0135593220339%\n",
      "Iteration: 84 | Batch: 4 | Cost: 0.05570784051112302 | Accuracy: 93.5406779661017%\n",
      "Iteration: 84 | Batch: 5 | Cost: 0.05366849137994141 | Accuracy: 93.27796610169491%\n",
      "Iteration: 85 | Batch: 1 | Cost: 0.07349187696141682 | Accuracy: 92.43898305084745%\n",
      "Iteration: 85 | Batch: 2 | Cost: 0.06910078556063382 | Accuracy: 91.87966101694916%\n",
      "Iteration: 85 | Batch: 3 | Cost: 0.09594506089034684 | Accuracy: 91.76440677966102%\n",
      "Iteration: 85 | Batch: 4 | Cost: 0.09466886370804825 | Accuracy: 91.29661016949152%\n",
      "Iteration: 85 | Batch: 5 | Cost: 0.10875073479997333 | Accuracy: 91.88474576271186%\n",
      "Iteration: 86 | Batch: 1 | Cost: 0.10766299522648208 | Accuracy: 92.94406779661017%\n",
      "Iteration: 86 | Batch: 2 | Cost: 0.07153718129013127 | Accuracy: 94.7864406779661%\n",
      "Iteration: 86 | Batch: 3 | Cost: 0.0504845216099765 | Accuracy: 96.03898305084746%\n",
      "Iteration: 86 | Batch: 4 | Cost: 0.0336251785725553 | Accuracy: 96.49491525423728%\n",
      "Iteration: 86 | Batch: 5 | Cost: 0.02545936697758023 | Accuracy: 96.51186440677965%\n",
      "Iteration: 87 | Batch: 1 | Cost: 0.029180641705130107 | Accuracy: 96.53559322033898%\n",
      "Iteration: 87 | Batch: 2 | Cost: 0.02200861350900645 | Accuracy: 96.5406779661017%\n",
      "Iteration: 87 | Batch: 3 | Cost: 0.028744635880131552 | Accuracy: 96.66949152542374%\n",
      "Iteration: 87 | Batch: 4 | Cost: 0.028226528344071734 | Accuracy: 96.65423728813559%\n",
      "Iteration: 87 | Batch: 5 | Cost: 0.022933234988878747 | Accuracy: 96.72881355932203%\n",
      "Iteration: 88 | Batch: 1 | Cost: 0.027077084797302364 | Accuracy: 96.62033898305084%\n",
      "Iteration: 88 | Batch: 2 | Cost: 0.020530922420984043 | Accuracy: 96.60847457627119%\n",
      "Iteration: 88 | Batch: 3 | Cost: 0.02693926393064027 | Accuracy: 96.79661016949153%\n",
      "Iteration: 88 | Batch: 4 | Cost: 0.02649590753063724 | Accuracy: 96.78813559322033%\n",
      "Iteration: 88 | Batch: 5 | Cost: 0.021731939275232832 | Accuracy: 96.80847457627118%\n",
      "Iteration: 89 | Batch: 1 | Cost: 0.025450392600402726 | Accuracy: 96.77966101694915%\n",
      "Iteration: 89 | Batch: 2 | Cost: 0.01928299734977844 | Accuracy: 96.7%\n",
      "Iteration: 89 | Batch: 3 | Cost: 0.025756458855170295 | Accuracy: 96.83898305084746%\n",
      "Iteration: 89 | Batch: 4 | Cost: 0.025369160738009867 | Accuracy: 96.82542372881356%\n",
      "Iteration: 89 | Batch: 5 | Cost: 0.020874077274308876 | Accuracy: 96.83050847457628%\n",
      "Iteration: 90 | Batch: 1 | Cost: 0.02465127706816094 | Accuracy: 96.71186440677965%\n",
      "Iteration: 90 | Batch: 2 | Cost: 0.018896340188497403 | Accuracy: 96.69491525423729%\n",
      "Iteration: 90 | Batch: 3 | Cost: 0.02493691081793135 | Accuracy: 96.87796610169491%\n",
      "Iteration: 90 | Batch: 4 | Cost: 0.02459871487173999 | Accuracy: 96.83220338983051%\n",
      "Iteration: 90 | Batch: 5 | Cost: 0.02047600735971529 | Accuracy: 96.8271186440678%\n",
      "Iteration: 91 | Batch: 1 | Cost: 0.023778296222928635 | Accuracy: 96.85762711864406%\n",
      "Iteration: 91 | Batch: 2 | Cost: 0.01795430464172574 | Accuracy: 96.80169491525423%\n",
      "Iteration: 91 | Batch: 3 | Cost: 0.024240530792707476 | Accuracy: 96.77118644067797%\n",
      "Iteration: 91 | Batch: 4 | Cost: 0.024181548165324362 | Accuracy: 96.73389830508474%\n",
      "Iteration: 91 | Batch: 5 | Cost: 0.02025102738760561 | Accuracy: 96.71694915254237%\n",
      "Iteration: 92 | Batch: 1 | Cost: 0.024180618594304742 | Accuracy: 96.40508474576272%\n",
      "Iteration: 92 | Batch: 2 | Cost: 0.019539289008501846 | Accuracy: 96.37627118644068%\n",
      "Iteration: 92 | Batch: 3 | Cost: 0.025908728567769778 | Accuracy: 96.38135593220339%\n",
      "Iteration: 92 | Batch: 4 | Cost: 0.02621934085317863 | Accuracy: 96.33898305084746%\n",
      "Iteration: 92 | Batch: 5 | Cost: 0.023067720761228493 | Accuracy: 96.26101694915255%\n",
      "Iteration: 93 | Batch: 1 | Cost: 0.02555685386982734 | Accuracy: 96.45254237288135%\n",
      "Iteration: 93 | Batch: 2 | Cost: 0.0192385027855582 | Accuracy: 96.55762711864406%\n",
      "Iteration: 93 | Batch: 3 | Cost: 0.025528151231730085 | Accuracy: 96.33389830508474%\n",
      "Iteration: 93 | Batch: 4 | Cost: 0.02589489508756752 | Accuracy: 96.41016949152542%\n",
      "Iteration: 93 | Batch: 5 | Cost: 0.021922827770328078 | Accuracy: 96.4271186440678%\n",
      "Iteration: 94 | Batch: 1 | Cost: 0.025763041650397715 | Accuracy: 95.93898305084745%\n",
      "Iteration: 94 | Batch: 2 | Cost: 0.022954383516032018 | Accuracy: 95.75423728813558%\n",
      "Iteration: 94 | Batch: 3 | Cost: 0.030455562974947458 | Accuracy: 95.70847457627119%\n",
      "Iteration: 94 | Batch: 4 | Cost: 0.03037544203302736 | Accuracy: 95.87627118644068%\n",
      "Iteration: 94 | Batch: 5 | Cost: 0.027240655218169353 | Accuracy: 95.78305084745763%\n",
      "Iteration: 95 | Batch: 1 | Cost: 0.028455261032280597 | Accuracy: 96.2322033898305%\n",
      "Iteration: 95 | Batch: 2 | Cost: 0.020949722496646898 | Accuracy: 96.53728813559323%\n",
      "Iteration: 95 | Batch: 3 | Cost: 0.025940902499930606 | Accuracy: 96.45423728813559%\n",
      "Iteration: 95 | Batch: 4 | Cost: 0.024955836699199513 | Accuracy: 96.64745762711865%\n",
      "Iteration: 95 | Batch: 5 | Cost: 0.020204812418786123 | Accuracy: 96.90508474576272%\n",
      "Iteration: 96 | Batch: 1 | Cost: 0.022539766308670377 | Accuracy: 96.79152542372881%\n",
      "Iteration: 96 | Batch: 2 | Cost: 0.017281039598744836 | Accuracy: 96.93559322033897%\n",
      "Iteration: 96 | Batch: 3 | Cost: 0.02191477918249875 | Accuracy: 97.06610169491525%\n",
      "Iteration: 96 | Batch: 4 | Cost: 0.02119310952318241 | Accuracy: 97.07288135593221%\n",
      "Iteration: 96 | Batch: 5 | Cost: 0.018070751787591193 | Accuracy: 97.11186440677966%\n",
      "Iteration: 97 | Batch: 1 | Cost: 0.020484972224718283 | Accuracy: 97.0457627118644%\n",
      "Iteration: 97 | Batch: 2 | Cost: 0.015363770107886972 | Accuracy: 97.12542372881356%\n",
      "Iteration: 97 | Batch: 3 | Cost: 0.020811404867121734 | Accuracy: 97.08813559322034%\n",
      "Iteration: 97 | Batch: 4 | Cost: 0.020414313931119636 | Accuracy: 97.09830508474576%\n",
      "Iteration: 97 | Batch: 5 | Cost: 0.017252659164712168 | Accuracy: 97.1593220338983%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 98 | Batch: 1 | Cost: 0.01997395054732167 | Accuracy: 97.03220338983051%\n",
      "Iteration: 98 | Batch: 2 | Cost: 0.015130840563336755 | Accuracy: 97.06101694915255%\n",
      "Iteration: 98 | Batch: 3 | Cost: 0.02038482671784301 | Accuracy: 97.14237288135593%\n",
      "Iteration: 98 | Batch: 4 | Cost: 0.019825686796839558 | Accuracy: 97.15762711864406%\n",
      "Iteration: 98 | Batch: 5 | Cost: 0.01682512625662371 | Accuracy: 97.18983050847457%\n",
      "Iteration: 99 | Batch: 1 | Cost: 0.019329395710288 | Accuracy: 97.08813559322034%\n",
      "Iteration: 99 | Batch: 2 | Cost: 0.014484429773775523 | Accuracy: 97.1593220338983%\n",
      "Iteration: 99 | Batch: 3 | Cost: 0.019843084289871676 | Accuracy: 97.09830508474576%\n",
      "Iteration: 99 | Batch: 4 | Cost: 0.019596707149857923 | Accuracy: 97.06271186440678%\n",
      "Iteration: 99 | Batch: 5 | Cost: 0.016793314138447077 | Accuracy: 97.14915254237289%\n",
      "Iteration: 100 | Batch: 1 | Cost: 0.01942170882117103 | Accuracy: 96.88305084745762%\n",
      "Iteration: 100 | Batch: 2 | Cost: 0.015227871636855772 | Accuracy: 96.95762711864407%\n",
      "Iteration: 100 | Batch: 3 | Cost: 0.020271711148074485 | Accuracy: 96.93728813559322%\n",
      "Iteration: 100 | Batch: 4 | Cost: 0.01951843925215745 | Accuracy: 97.1135593220339%\n",
      "Iteration: 100 | Batch: 5 | Cost: 0.016734040658800856 | Accuracy: 97.06440677966101%\n"
     ]
    }
   ],
   "source": [
    "N = Xtrain.shape[0] # Num samples in training data\n",
    "D = Xtrain.shape[1] # Num input features\n",
    "batch_size=10000 # Batch Size\n",
    "n_batches = N // batch_size # Num batches\n",
    "\n",
    "M = 100 # hidden layer nodes\n",
    "K = 10 # Num Output features\n",
    "lr = 0.01 # Learning Rate\n",
    "\n",
    "momentum = 0.99 # Momentum const\n",
    "decay = 0.999 # Decay const\n",
    "epsilon = 1e-8 # Tiny positive value\n",
    "iterations = 100 # Num iterations\n",
    "\n",
    "Ctrain = [] # Train Cost\n",
    "np.random.seed(1) # Random seed for Weights\n",
    "\n",
    "T = oneHotEncode(Ytrain) # Apply One Hot Encoding for Training Outputs\n",
    "\n",
    "Xm = th.tensor.matrix('Xtrain') # Input Tensor\n",
    "Ym = th.tensor.matrix('T') # Output Tensor\n",
    "\n",
    "W1_init = np.random.randn(D,M) # Weight initialization for W between input and hidden\n",
    "b1_init = np.random.randn(M) # Weight initialization for Bias at hidden layer\n",
    "W2_init = np.random.randn(M,K) # Weight initialization for W between hidden and output\n",
    "b2_init = np.random.randn(K) # Weight initialization for Bias at output layer\n",
    "\n",
    "W1 = th.shared(W1_init, \"w1\") # Shared Variable W1\n",
    "b1 = th.shared(b1_init, \"b1\") # Shared Variable b1\n",
    "W2 = th.shared(W2_init, \"w2\") # Shared Variable W2\n",
    "b2 = th.shared(b2_init, \"b2\") # Shared Variable b2\n",
    "\n",
    "Vw1 = th.shared(np.zeros((D,M)), \"Vw1\") # Shared Variable Velocity Vw1 \n",
    "Vb1 = th.shared(np.zeros((M)), \"Vb1\") # Shared Variable Velocity Vb1\n",
    "Vw2 = th.shared(np.zeros((M,K)), \"Vw2\") # Shared Variable Velocity Vw2\n",
    "Vb2 = th.shared(np.zeros((K)), \"Vb2\") # Shared Variable Velocity Vb2\n",
    "\n",
    "cache_w1 = th.shared(np.zeros((D,M)), \"cache_w1\") # Shared Variable Cache_w1\n",
    "cache_b1 = th.shared(np.zeros((M)), \"cache_b1\") # Shared Variable Cache_b1\n",
    "cache_w2 = th.shared(np.zeros((M,K)), \"cache_w2\") # Shared Variable Cache_w2\n",
    "cache_b2 = th.shared(np.zeros((K)), \"cache_b2\") # Shared Variable Cache_b2\n",
    "\n",
    "Z1 = th.tensor.nnet.relu(th.tensor.dot(Xm,W1)+b1) # Dot prodcut with input to hidden and apply relu\n",
    "Z2 = th.tensor.nnet.softmax(th.tensor.dot(Z1,W2)+b2) # Dot product with hidden to output and apply softmax\n",
    "\n",
    "cost = -(Ym*th.tensor.log(Z2)).mean() # Cost Function\n",
    "predict = th.tensor.argmax(Z2, axis=1) # Predict function\n",
    "t = th.shared(1, \"t\") # Timestep for Adam\n",
    "\n",
    "# momentum term\n",
    "Vw1_update = Vw1 * momentum + (1 - momentum) * th.tensor.grad(cost,W1)\n",
    "Vb1_update = Vb1 * momentum + (1 - momentum) * th.tensor.grad(cost,b1)\n",
    "Vw2_update = Vw2 * momentum + (1 - momentum) * th.tensor.grad(cost,W2)\n",
    "Vb2_update = Vb2 * momentum + (1 - momentum) * th.tensor.grad(cost,b2)\n",
    "\n",
    "# Bias correction\n",
    "Vw1_update_correct = Vw1_update / (1 - momentum**t)\n",
    "Vb1_update_correct = Vb1_update / (1 - momentum**t)\n",
    "Vw2_update_correct = Vw2_update / (1 - momentum**t)\n",
    "Vb2_update_correct = Vb2_update / (1 - momentum**t)\n",
    "\n",
    "# RMSProp term\n",
    "cache_w1_update = cache_w1 * decay + (1 - decay) * th.tensor.grad(cost,W1)**2\n",
    "cache_b1_update = cache_b1 * decay + (1 - decay) * th.tensor.grad(cost,b1)**2\n",
    "cache_w2_update = cache_w2 * decay + (1 - decay) * th.tensor.grad(cost,W2)**2\n",
    "cache_b2_update = cache_b2 * decay + (1 - decay) * th.tensor.grad(cost,b2)**2\n",
    "\n",
    "# Bias correction\n",
    "cache_w1_update_correct = cache_w1_update / (1 - decay**t)\n",
    "cache_b1_update_correct = cache_b1_update / (1 - decay**t)\n",
    "cache_w2_update_correct = cache_w2_update / (1 - decay**t)\n",
    "cache_b2_update_correct = cache_b2_update / (1 - decay**t)\n",
    "\n",
    "# Weights Update with ADAM\n",
    "W1_update = W1 - lr * th.tensor.grad(cost,W1) / (th.tensor.sqrt(cache_w1_update_correct) + epsilon)\n",
    "b1_update = b1 - lr * th.tensor.grad(cost,b1) / (th.tensor.sqrt(cache_b1_update_correct) + epsilon)\n",
    "W2_update = W2 - lr * th.tensor.grad(cost,W2) / (th.tensor.sqrt(cache_w2_update_correct) + epsilon)\n",
    "b2_update = b2 - lr * th.tensor.grad(cost,b2) / (th.tensor.sqrt(cache_b2_update_correct) + epsilon)\n",
    "\n",
    "# Train func\n",
    "train = th.function(inputs=(Xm,Ym), outputs=cost, updates=[(W1, W1_update),\n",
    "                                                           (b1, b1_update),\n",
    "                                                           (W2, W2_update),\n",
    "                                                           (b2, b2_update),\n",
    "                                                           (Vw1, Vw1_update),\n",
    "                                                           (Vb1, Vb1_update),\n",
    "                                                           (Vw2, Vw2_update),\n",
    "                                                           (Vb2, Vb2_update),\n",
    "                                                           (cache_w1, cache_w1_update),\n",
    "                                                           (cache_b1, cache_b1_update),\n",
    "                                                           (cache_w2, cache_w2_update),\n",
    "                                                           (cache_b2, cache_b2_update),\n",
    "                                                           (t,t+1)])\n",
    "\n",
    "# Get prediction func\n",
    "get_predict = th.function(inputs=[Xm,Ym], outputs=[cost,predict])\n",
    "\n",
    "# Iterations with Mini Batches\n",
    "for i in range(iterations+1):\n",
    "    for j in range(n_batches):\n",
    "        c = train(Xtrain[j*batch_size:(j+1)*batch_size],T[j*batch_size:(j+1)*batch_size]) # Mini Batch training\n",
    "        _, yhat = get_predict(Xtrain, T) # Prediction for whole Training Data\n",
    "        \n",
    "        Ctrain.append(c)\n",
    "        print(\"Iteration: \"+ str(i)+ \" | Batch: \" + str(j+1) + \" | Cost: \"+ str(c) + \" | Accuracy: \" + str(accuracy(Ytrain,yhat)*100) +\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAFlCAYAAADCjqI2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfSUlEQVR4nO3deZDcZ53f8c/39+tjZrrn0FySrMMj2QLfYHsAgw0Bm4DXy5HdQMFuLVkIRFspWNhKKgR2qyi2kkrIZmuPqq0EvFzO2gshsC4cWDAOYBwTfIwsG0uWfOq+ZnTNffTxzR/dM+rR6BhL6ulnut+vqqnpa9rfpxGfeeb7e36/x9xdAIBwRbUuAABwbgQ1AASOoAaAwBHUABA4ghoAAkdQA0DgEtV40+7ubu/r66vGWwNAXdqyZctRd+8503NVCeq+vj4NDAxU460BoC6Z2Z6zPUfrAwACR1ADQOAIagAIHEENAIEjqAEgcAQ1AASOoAaAwBHUABA4ghoAAkdQA0DgCGoACFxQQf3IC0N6aXCs1mUAQFCCCurNfzeg7wzsq3UZABCURQW1mXWY2XfNbKeZ7TCzN1ejmNhMhSK7ogNApcVe5vSvJf3Y3T9gZilJLdUoJopMRSeoAaDSeYPazNokvU3SRyXJ3WckzVSjmMhMRWbUADDPYlofGyUNSfqGmW01s6+aWeb0F5nZZjMbMLOBoaGhCyomjkwFZtQAMM9igjoh6SZJ/93db5Q0Lulzp7/I3e9293537+/pOeNuMucvxkyF4gX9KADUrcUE9X5J+9398fL976oU3JdcHEnOjBoA5jlvULv7YUn7zOy15YfukPRcVYph1QcALLDYVR9/KOm+8oqPVyR9rBrFREaPGgBOt6igdvenJfVXt5TSwURWfQDAfEGdmRhHJnIaAOYLKqjNROsDAE4TVFDHnPACAAuEFdQRqz4A4HRBBbUZPWoAOF1QQR1H4qJMAHCasIKaE14AYIGggprLnALAQmEFtRHUAHC6oIKa1gcALBRUUEeRVOQypwAwT1BBHdOjBoAFggpqrp4HAAsFF9ScQg4A8wUV1OyZCAALBRXUpRl1rasAgLAEFtScQg4ApwsqqLl6HgAsFFRQR/SoAWCBsILaTOQ0AMwXVFDHJlofAHCaoII6okcNAAsEFdSxmZzeBwDME1RQcwo5ACwUVlBHpgInvADAPEEFNXsmAsBCYQU1O7wAwAJBBbWxwwsALBBUUMcRlzkFgNMFF9Ss+gCA+YIKajOJCTUAzBdUUMfs8AIAC4QV1LQ+AGCBoIJ69up5nEYOAKcEF9QSfWoAqBRUUMflalhLDQCnBBXUUTQ7oyaoAWBWUEEdG0ENAKcLKqhne9S0PgDglMRiXmRmuyWNSipIyrt7fzWKmWt9cKlTAJizqKAue4e7H61aJSrtmSiJtdQAUCGo1kfMwUQAWGCxQe2SfmJmW8xsc7WKsdmDifSoAWDOYlsft7r7QTPrlfSQme1090cqX1AO8M2StH79+gsqZnZGTesDAE5Z1Iza3Q+Wvw9Kul/SG8/wmrvdvd/d+3t6ei6omJhVHwCwwHmD2swyZtY6e1vSuyRtq0Yx5ZwWE2oAOGUxrY+Vku4v948Tkv7e3X9cjWLmWh/MqAFgznmD2t1fkfS6JaiFHjUAnEGYy/OYUQPAnKCCOlEO6jxBDQBzggpqrvUBAAsFFdSJmBk1AJwuqKCOo1I5zKgB4JSwgprWBwAsEFZQzx1M5DqnADArqKCe7VGT0wBwSlBBzYwaABYKK6jpUQPAAmEFNdf6AIAFggrq2R41QQ0ApwQV1LOtD054AYBTwgpq9kwEgAWCCupE+czEfIGgBoBZQQV1OafpUQNAhaCCenZGzcYBAHBKUEEdcz1qAFggyKAuFDgzEQBmhRnUTKgBYE5QQZ2YOzORGTUAzAoqqOlRA8BCQQY1u5ADwClhBTWnkAPAAkEFdRSZzDjhBQAqBRXUUumAIjNqADgluKCOI6NHDQAVwgtqY0YNAJXCC+rI6FEDQIXggjoRRwQ1AFQILqhjDiYCwDzhBbUZp5ADQIXwgjoycfE8ADgluKBOxMyoAaBScEHN8jwAmC+8oI6MXcgBoEKQQc0u5ABwSpBBzTpqADgluKBORMYu5ABQIbigZkYNAPMtOqjNLDazrWb2g2oWRI8aAOZ7NTPqz0jaUa1CZsW0PgBgnkUFtZmtlfSbkr5a3XKkRMRFmQCg0mJn1H8l6bOSqn7KYMRFmQBgnvMGtZm9R9Kgu285z+s2m9mAmQ0MDQ1dcEGJiFPIAaDSYmbUt0p6n5ntlvRtSbeb2b2nv8jd73b3fnfv7+npueCCuCgTAMx33qB298+7+1p375P0YUk/c/ffq1ZBXOYUAOYLbx11TI8aAColXs2L3f1hSQ9XpZKyBLuQA8A84c2oucwpAMwTXlBzCjkAzBNcUJd2eCGoAWBWcEHNjBoA5gsvqOlRA8A84QV1FLHqAwAqBBfUCdZRA8A8wQV1ZPSoAaBScEHNVlwAMF9wQT276sMJawCQFGhQS6L9AQBl4QY1M2oAkBRgUCeYUQPAPMEF9eyMmiV6AFASbFBz0gsAlAQX1Alm1AAwT3BBHdGjBoB5ggtqZtQAMF9wQR1HpZLoUQNASYBBXfrOjBoASgIM6lJJhWKxxpUAQBiCC+pTJ7zUuBAACERwQR3Z7MFEkhoApACDmlPIAWC+4II6jglqAKgUXFAzowaA+YIL6tg44QUAKoUX1FyUCQDmCS6oEzEzagCoFFxQzy7Po0cNACXBBXVi7sxEghoApACDmh1eAGC+YIOaGTUAlIQb1OxCDgCSAgzqZHnVRy7PtT4AQAowqNOJWJI0w+XzAEBSgEGdSpRKms4ValwJAIQhuKBOzwY1rQ8AkBRwUM8Q1AAgKcCgTsSRImNGDQCzzhvUZtZkZk+Y2TNmtt3M/rTaRaUTsabz9KgBQJISi3jNtKTb3X3MzJKSHjWzH7n7Y9UqKp2MaH0AQNl5g9rdXdJY+W6y/FXVs1FScUTrAwDKFtWjNrPYzJ6WNCjpIXd/vJpFpZMENQDMWlRQu3vB3V8vaa2kN5rZdae/xsw2m9mAmQ0MDQ1dVFHpREzrAwDKXtWqD3c/KelhSXee4bm73b3f3ft7enouqqhS64ODiQAgLW7VR4+ZdZRvN0t6p6Sd1SyK1gcAnLKYVR+rJd1jZrFKwf4dd/9BNYviYCIAnLKYVR+/lnTjEtQyJ52MNTyZW8r/JAAEK7gzE6XSaeQcTASAkiCDOpXgYCIAzAoyqNOJSNM5ZtQAIAUb1DEbBwBAWaBBHbFxAACUhRvUHEwEAEkBB/VMoShnJ3IACDOoU4lI7lKuQFADQJBBPbsTOUv0ACDUoE6ybyIAzAoyqFMxO5EDwKwgg3p2Rk1QA0CgQZ2KSz1qWh8AEGhQpxOzM2oOJgJAmEHNwUQAmBNkUHMwEQBOCTKo00nWUQPArDCDOkHrAwBmBRnUqQStDwCYFWRQz636YPMAAAg1qMs9ajYPAIAwg3qu9cHmAQAQZlCn6VEDwJwgg3p2HTWrPgAg0KCOIlMqZjsuAJACDWqp1P6YokcNAOEGdUs61uQMQQ0AwQZ1Jp3Q2Ey+1mUAQM2FG9SphManCWoACDeo0zFBDQAKOKiz6YTGp+lRA0CwQZ1JJzROjxoAwg3qFnrUACAp4KDOpmONEdQAEG5QZ9IJTeWKKhS91qUAQE0FG9TZdEKS6FMDaHjBBnVLqhzUtD8ANLhggzqTLm0eQFADaHTBBvVc64O11AAaXLBBnUnT+gAAaRFBbWbrzOznZrbDzLab2WeWorBMuUfNEj0AjS6xiNfkJf1bd3/KzFolbTGzh9z9uWoWNtejZtUHgAZ33hm1ux9y96fKt0cl7ZC0ptqF0aMGgJJX1aM2sz5JN0p6vCrVVKBHDQAliw5qM8tK+p6kP3L3kTM8v9nMBsxsYGho6KILa06yPA8ApEUGtZklVQrp+9z9H870Gne/29373b2/p6fn4guLTJlUrDFaHwAa3GJWfZikr0na4e5/Uf2STsmkE5rgYCKABreYGfWtkj4i6XYze7r8dVeV65JUOqDI8jwAje68y/Pc/VFJtgS1LNDCdlwAEO6ZidLsBrf0qAE0tqCDOst2XAAQdlBn0mzHBQCBBzXL8wAg7KBOsTwPAMIO6nRCEzMFFdk3EUADCzqo2TcRAAIP6pa57bjoUwNoXEEHNTNqAAg8qDPsRA4AgQd1mu24ACDwoKZHDQCBBzWtDwAIOqhXtjVJkg6cnKxxJQBQO0EHdTad0Or2Jr08OFbrUgCgZoIOakm6sjerl4YIagCNK/igvqInq5cHx+TOaeQAGlPwQb26vUnjMwWNz7DyA0BjCj6ou7NpSdLR0ekaVwIAtRF8UHdlU5KkY+MENYDGFHxQz86oh0ZnalwJANRG8EHd01pufYwxowbQmIIP6s5MufUxxowaQGMKPqiTcaSOliQzagANK/iglkp9aoIaQKNaJkGdovUBoGEti6DuYkYNoIEti6DuyaY1RFADaFDLIqi7symNTuU1leM0cgCNZ1kEdVf5pJfj4/SpATSeZRHUPXNnJ9L+ANB4lkVQr2ov7fRyeGSqxpUAwNJbFkE9uyXX4WGCGkDjWRZB3ZVJKRkbM2oADWlZBHUUmXpbm3SEGTWABrQsgloq9akPEdQAGtCyCereVk56AdCYlk1Qd2VTOkZQA2hAyyaou7NpnZjIKVco1roUAFhSyyaoZ89OPMHZiQAazHmD2sy+bmaDZrZtKQo6m57yJrf0qQE0msXMqL8p6c4q13FeszNqrksNoNGcN6jd/RFJx5eglnPq5nofABrUsulRr+loVksq1tP7Tta6FABYUpcsqM1ss5kNmNnA0NDQpXrbOalEpDdt6NSjLx295O8NACG7ZEHt7ne7e7+79/f09Fyqt53ntk092nV0XPtPTFTl/QEgRMum9SFJb9vULUl69EVm1QAax2KW531L0q8kvdbM9pvZx6tf1pld2ZtVb2taj71yrFYlAMCSS5zvBe7+O0tRyGKYmW5Y265tB0dqXQoALJll1fqQpOvWtOvloTFNzORrXQoALInlF9SXtctd2nGIWTWAxrD8gnpNuyTp2f3DNa4EAJbGsgvqlW1pdWdT9KkBNIxlF9RmpuvWtGvbAWbUABrDsgtqqdSnfnFwTFO5Qq1LAYCqW55BvaZNhaJr5+HRWpcCAFW3LIP6+rUdkqT/9MMdmsmz4wuA+rYsg3pNR7P+w/uv1RO7j+srv3i51uUAQFUty6CWpI+8uU9v3dSt/7Vlv9y91uUAQNUs26CWpLuuX629xyf0LCtAANSx5R3U161WczLWfY/trXUpAFA1yzqo21uS+o3rVuknzx2m/QGgbi3roJakWzZ26cRETi8Pjde6FACoimUf1P19KyRJv/u3j+k7A/tqXA0AXHrLPqg39mT1x3ddpcHRaf3XB59XrsC6agD1ZdkHtSRtftsV+sbH3qCh0Wl9+WHWVQOoL3UR1JL09tf06F3XrNTdj7zCrBpAXamboDYz/fZNazU6ndef/u/tembfyVqXBACXRN0EtSTdtqlbLalY9z62V7/1336pwdGpWpcEABetroI6m07ogU/dqvfcsFpFlz78lcf00x1HmF0DWNasGieK9Pf3+8DAwCV/31fjgWcO6tPf2jp3/wd/eNvcNl4AEBoz2+Lu/Wd6rq5m1JXec/1qffQtfbpqVask6V98/QkOMgJYlhK1LqBaosj0xfddK0l6cPth/cHfbdGXfrRTd163Sm/o66xxdQCweHU7o6709tf2SJK+9uguffDLv9L/+NXu2hYEAK9CQwR1OhHrj++6SpLUlUnpC9/frr7P/VBf+tHOGlcGAOdXt62P0/2rt27Uh/rXqyUd6xP3DOgXLwzpy794Wa1NCX3yHVfWujwAOKu6XfVxLvlCUdsPjujfffcZvXBkTH1dLVqzolmfvn2T3rSxq9blAWhA51r10ZBBPWtsOq8bvvigihUfQXc2pbuuX60vvvdaRZHVrjgADYWgPodtB4b1P5/cpz/4Jxv1iXsGtPPwqCSpM5PSJ966QYdOTuntr+3RHVevrHGlAOoZQb1IU7mCntp7Qg9uO6x7frVn3nOXd7Xo6lVt+uQ7rtRrVmXlLjUl4xpVCqDenCuoG+Zg4mI0JWO95YpuveWKbt2ysUv3Pb5Xn75jk/78J8/riV3HtefYhH68/fDc69euaNbdH+lXR0tSl3U017ByAPWMGfUiFIuufScmtHXvSX3pRzt1eGThxZ5et7Zd//zmtXru4IhuunyFPnDTWpmVruoHAOdD6+MSmskXte3gsG5c16Fn9g/rzx98Xo++dPSsr7/jql6993WX6fFdx7W6vUm3berW97ce0NZ9J/XRt/TphrXtWt+ZUSrREEvaAZwFQb0EXjgyqnsf26Nbr+zWM/tO6iuPvKJCcfGf7d/87o3qaE5Jkt5yRRcrToAGQ1DXyPBETkNjU7r7kVf01k09yqRjPbn7hD5w81olo0gf++YTOjY+o5MTuXk/l4hMl3U068rerN77utV6eXBcUWT64M1rJUmtTQm1NSV1aGRKT+46rndfu0pNyYg2C7CMEdSBGxyd0jd+uVuHh6c0NDp9zlbKmaQTkabzRW3qzeq6Ne3admBYuUJRb3tNj7qzaa3vbNGbr+jSiYkZdbak1J1Na2wmr+GJnHpa06xeAQJAUC8zJ8ZnJEkFd31nYJ9am5Ja3dak+58+oJeOjKmjJalnDwyrozmpN1/Rrb3Hx/Xk7hOLfv+mZKSp3PxLvl69uk03X96hQtHVmUnp9qt61dGSUjadUHc2rULRX1UffXgip18fOKl0ItYbN1zc1Qpn/41OzBSUSbNQqRE8u39YR0amdMfVvQ3zlyJBXYeKRZ/Xx57KFRRHpl++dFSvWdmqrmxK2w6MaNuBYY1N57VlzwkdODGpa9e0aWQyp+PjM1rd0axHnh/S6HT+nP+tRGTKF11m0lWr2uTucpeuXdOmAycmFZlpfWeLjk/MaN/xCUVmeu7QyLz3SCUi3XntKm3qzaq/r1NX9GbUk03PPW9mms4X5C49tfeEBkemdXlXi57Zd1J/+3936cjIlPJF18q2tL702zfoHVf1XtoPFEGYyhX0yfue0k93DkqSvvHRNzTM/9YENc5qbDqv5mSsyKQ9xyY0Np1X0V33bz2ggd0n1NOa1tDotJ49MKy+rhYNjk5rYqZw1vdrScVa09GsdDLSWzf16MUjo/rZzkG5pLP9U8umExqfyZ/1eTPp47du0E+eO6K9xyckSVetatUVvVndsqFTMlNXJqX25qQ6MyklY9N0vqhEFCkRm3KForqzaU1MFzQ8mdPYdF7/Z8cRjUzmlIgjdWaSurI3q2sva1dzMlYiNq1uZ138Ujs8PKX3/s2jGhqd1o3rO7R170n91o1r9Jcfen2tS1sSFx3UZnanpL+WFEv6qrt/6VyvJ6jr18hUTu6lGf3YdF6J2JSIIg2OTslkamtOaO2KljP+7HMHR7T94LD2n5jU84dH9dOdR+baK7uOjmtjT0b9l6/QiYmcNvZkJJdc0r+8dYNWtTepUHS9MjSmz37v19q692RVx9mVSWl8Jq/V7c1qTsY6ODypvq6MmpOx9p+c0LoVLWpJJfTK0TFFZkonIr00OKZ1nS26oiejXUfHNTQ6rRWZlEYmc7pqVZv6ultUKLoKRVd3Nq29xyfUlIy1bkWLsk0JzeSLWt3epHQiUsFdt2zsUjad0J5jE1rV1qTpfEFDY9N6zcpWJeNII1M5ZVIJDY1O65EXhrSqvUmXdTRraHRa67tatPfYhAZ2H1fBXVv3ntTYdF6vX9ehbDqh4cmc+vtW6ObLV+jkRE4bujM1O1bh7vrm/9ut//jDHSoUXW/a0Klvb75FX3xgu+59fK++/8lbG2IbvYsKajOLJb0g6Z9K2i/pSUm/4+7Pne1nCGosVuW/v8X2InOFok5O5LTr6Lgik3YeHtWxsRm1NSf0s52D6s6mtaE7o6f2nlBXJq2VbWk9ufu4ujJp3Xz5CknSxp6Mrl7dpmf2ndT4TEFtTQk9ufu4jo/nNJUr6MHth1Vw1/rOFu05VprFd2VSOlY+fpBKRJrJl/r86zqbdeDEpIouxZHNLcvsv3yF9hyf0NDotJqTsSZzZ/9LpNZScaTetrSmcgW1NScVmykRR+rralEqEWl8uqCOlqTcS+2J9pakTNLwZE5tzUlNzhS0+9i4suVjCCcmZrS+s0WRmfYcm9C6zmatbGvSVK6o1qaEIjNN5QpKxqZfvnRMzx0a0eVdLfrUO67UB/vXSSod53jnX/5C2XRCH3rDOpmkTDqhjpZk6ZhJHKm9fFsq/WXmkgpFV2tTQibT+ExeqTjS8GROo1M5ZdNJRZE0MV1qFT5/ZFTDkzmtbE2ruzWtjuaU1nU2q705qVzB1d6clMs1ky8qk0qo6K580dWUjFUslm6nEtFcDfFFLKu92KB+s6Qvuvu7y/c/L0nu/p/P9jMENepBrlBUMo6ULxQ1Np1Xe3NSknRkZFqdmZSOjEwpk06oM1Na/z6TL8rl2nNsQhu6M0rGpYOvU+WA/vX+YTUlI129uk0T0wW9ODiqTb2tenzXMU3mCnr9ug7tODSiIyPTWtnWpMd3HdPeYxO6fm27joxMaWQqr6tWtmrn4VG9PDSmG9ev0OhUTs3JWO++dpXGpvN6et9JXdGb1eDIlFa0pHTH1b1akUlp99FxXd6Z0dDYtNKJUsD9fOegntpzQisyKT13cESHR6bU25rWkZFp7Tk2rlXtTdpzbELT+aLamhKamCkoX3Rl0wlN5goqFF0tqVjT+aIKRdfaFc2ayhU1lSuoK1v6fNylNSuadWR4SuMzBSUiU9FdRS+1tKTSpRjuun61/v27r1pw/sDDzw/qX9/7VFV/yVX+cj2X2XrdS7+o5VKuWFQyilT00jGcTb2t+uGnb7ugA6AXG9QfkHSnu3+ifP8jkt7k7p86288Q1EB9cHcNT+bUlIwVR6Z8wRWXwzZXKCqViErBFUcLQrZQdEXlyygUyq2yyKRisbSiKRmbWlKJ885CR6Zy2n98UkV3RWbaf2JCqUSkdCLWoeFJJeJI2XSsIyPTiiNTW1NSh4cnZWbqzqY1MpVTSyrW+s4WHR6e0mSuoMs6mjWVK6inNa0re7Pae2xCe45NKIqkXUcnNDKZU2tTqa00PpPXqrYmHR2b0VSuoN62Jh0bKx2rWdPRrJGp0l9hKzIp5fKuL7z3mgv6rC/2okxn+hQXpLuZbZa0WZLWr1//qgoEECYzU0dLau5+ZRv7fD3tygCOI5v7i+TVamtK6prLTv3sNZe1XdD7nMumla3atLL1kr/vpbKYhbH7Ja2ruL9W0sHTX+Tud7t7v7v39/T0XKr6AKDhLSaon5S0ycw2mFlK0oclPVDdsgAAs87b+nD3vJl9StKDKi3P+7q7b696ZQAASYvcOMDd/1HSP1a5FgDAGXARZAAIHEENAIEjqAEgcAQ1AASOoAaAwBHUABA4ghoAAkdQA0DgCGoACFxVtuIysyFJey7wx7slvbptuJe3Rhuv1HhjZrz171KM+XJ3P+MV7aoS1BfDzAbOdk3WetRo45Uab8yMt/5Ve8y0PgAgcAQ1AAQuxKC+u9YFLLFGG6/UeGNmvPWvqmMOrkcNAJgvxBk1AKBCMEFtZnea2fNm9pKZfa7W9VwqZvZ1Mxs0s20Vj3Wa2UNm9mL5+4qK5z5f/gyeN7N316bqC2dm68zs52a2w8y2m9lnyo/X5ZjNrMnMnjCzZ8rj/dPy43U53llmFpvZVjP7Qfl+vY93t5k9a2ZPm9lA+bGlG7O71/xLpS2+Xpa0UVJK0jOSrql1XZdobG+TdJOkbRWP/Zmkz5Vvf07SfynfvqY89rSkDeXPJK71GF7leFdLuql8u1XSC+Vx1eWYJZmkbPl2UtLjkm6p1/FWjPvfSPp7ST8o36/38e6W1H3aY0s25lBm1G+U9JK7v+LuM5K+Len9Na7pknD3RyQdP+3h90u6p3z7Hkn/rOLxb7v7tLvvkvSSSp/NsuHuh9z9qfLtUUk7JK1RnY7ZS8bKd5PlL1edjleSzGytpN+U9NWKh+t2vOewZGMOJajXSNpXcX9/+bF6tdLdD0mlYJPUW368rj4HM+uTdKNKs8y6HXO5DfC0pEFJD7l7XY9X0l9J+qykYsVj9TxeqfTL9ydmtsXMNpcfW7IxL2pz2yVgZ3isEZej1M3nYGZZSd+T9EfuPmJ2pqGVXnqGx5bVmN29IOn1ZtYh6X4zu+4cL1/W4zWz90gadPctZvb2xfzIGR5bNuOtcKu7HzSzXkkPmdnOc7z2ko85lBn1fknrKu6vlXSwRrUshSNmtlqSyt8Hy4/XxedgZkmVQvo+d/+H8sN1PWZJcveTkh6WdKfqd7y3Snqfme1WqUV5u5ndq/odryTJ3Q+Wvw9Kul+lVsaSjTmUoH5S0iYz22BmKUkflvRAjWuqpgck/X759u9L+n7F4x82s7SZbZC0SdITNajvgllp6vw1STvc/S8qnqrLMZtZT3kmLTNrlvROSTtVp+N198+7+1p371Pp/6c/c/ffU52OV5LMLGNmrbO3Jb1L0jYt5ZhrfTS14gjqXSqtEHhZ0p/Uup5LOK5vSTokKafSb9qPS+qS9FNJL5a/d1a8/k/Kn8Hzkn6j1vVfwHhvU+nPvF9Lerr8dVe9jlnSDZK2lse7TdIXyo/X5XhPG/vbdWrVR92OV6XVaM+Uv7bP5tNSjpkzEwEgcKG0PgAAZ0FQA0DgCGoACBxBDQCBI6gBIHAENQAEjqAGgMAR1AAQuP8PaYxTDaad3uEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cost plot\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(Ctrain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
